{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MORDM optimization\n",
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-16T11:39:25.110002Z",
     "end_time": "2023-06-16T11:39:26.348399Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Logger EMA (DEBUG)>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from problem_formulation import get_model_for_problem_formulation\n",
    "from ema_workbench import ema_logging, MultiprocessingEvaluator, Scenario, HypervolumeMetric\n",
    "from ema_workbench.em_framework.optimization import (ArchiveLogger, EpsilonProgress, to_problem)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "ema_logging.log_to_stderr(ema_logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare model and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-16T11:39:26.350302Z",
     "end_time": "2023-06-16T11:39:26.479480Z"
    }
   },
   "outputs": [],
   "source": [
    "model, planning_steps = get_model_for_problem_formulation(2)\n",
    "\n",
    "#The model requires a reference scenario to \"kick-start\"\n",
    "reference_scenario = Scenario('reference', **{\n",
    "    'discount rate-0': 1.5,\n",
    "    'discount rate-1': 1.5,\n",
    "    'discount rate-2': 1.5,\n",
    "    'A.0_ID flood wave shape': 75,\n",
    "    'A.1_Bmax': 240,\n",
    "    'A.1_pfail': 0.25,\n",
    "    'A.1_Brate': 10,\n",
    "    'A.2_Bmax': 240,\n",
    "    'A.2_pfail': 0.25,\n",
    "    'A.2_Brate': 10,\n",
    "    'A.3_Bmax': 240,\n",
    "    'A.3_pfail': 0.25,\n",
    "    'A.3_Brate': 10,\n",
    "    'A.4_Bmax': 240,\n",
    "    'A.4_pfail': 0.25,\n",
    "    'A.4_Brate': 10,\n",
    "    'A.5_Bmax': 240,\n",
    "    'A.5_pfail': 0.25,\n",
    "    'A.5_Brate': 10\n",
    "})\n",
    "\n",
    "nfe = 75000\n",
    "epsilon = [0.5,0.5,0.5,0.01,0.01]\n",
    "\n",
    "# Each epsilon value corresponds to a model outcome. The model outcomes are:\n",
    "# expected damages, dike investment costs, rfr costs, evacuation cost, and casualties\n",
    "# We select higher epsilon values to damages and costs, while we choose lower values for evacuation costs and casualties.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypervolume and epsilon progress as convergence metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-16T11:35:26.788330Z",
     "end_time": "2023-06-16T11:37:12.723483Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MainProcess/INFO] pool started with 10 workers\n",
      " 46%|████████████████▍                   | 34240/75000 [12:10<15:05, 44.99it/s]"
     ]
    }
   ],
   "source": [
    "# Create the data folder if it doesn't exist, remove temp folder if it exists\n",
    "Path(\"./MORDM_Data\").mkdir(parents=True, exist_ok=True)\n",
    "tmppath = Path('./MORDM_Data') / 'tmp'\n",
    "if tmppath.exists() and tmppath.is_dir():\n",
    "    shutil.rmtree(tmppath)\n",
    "\n",
    "# Define the convergence metrics\n",
    "convergence_metrics = [\n",
    "    # Save data as archive, for hypervolume metric\n",
    "    ArchiveLogger(\n",
    "        \"./MORDM_Data\",\n",
    "        [l.name for l in model.levers],\n",
    "        [o.name for o in model.outcomes],\n",
    "        base_filename=\"MORDM_HV_1.tar.gz\",\n",
    "    ),\n",
    "    # Track epsilon progress\n",
    "    EpsilonProgress(),\n",
    "]\n",
    "\n",
    "with MultiprocessingEvaluator(model) as evaluator:\n",
    "    optimization_result, optimization_convergence = evaluator.optimize(nfe=nfe, searchover='levers', epsilons=epsilon,  convergence=convergence_metrics,reference=reference_scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Constrain results\n",
    "\n",
    "Filtering the found solutions to those that are most important for the Delta Commission. Deaths should not exceed 0.001 for the entire territory, as well as adaptability measured in the form of a continuous set of measures that in the long run make sense (e.g., not following dike heightening with RfR to avoid losing the investment of the first timestep)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Select only those policies that result in an tolerable # of deaths\n",
    "result_constrained = optimization_result[optimization_result['Expected Number of Deaths'] < 0.001]\n",
    "result_constrained"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-16T11:37:12.724544Z",
     "end_time": "2023-06-16T11:37:12.732596Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def has_rfr_after_dh(series):\n",
    "    for area in range(5):\n",
    "        first_rfr_timestep = -1\n",
    "        for timestep in range(3):\n",
    "            if series[f\"{area}_RfR {timestep}\"] > 0:\n",
    "                first_rfr_timestep = timestep\n",
    "                break\n",
    "        last_dike_timestep = 3\n",
    "        for timestep in range(3):\n",
    "            if series[f\"A.{area + 1}_DikeIncrease {timestep}\"] > 0:\n",
    "                last_dike_timestep = timestep\n",
    "        print(f\"First RfR timestep: {first_rfr_timestep}; last DH timestep: {last_dike_timestep}\")\n",
    "        if first_rfr_timestep > last_dike_timestep:\n",
    "            print(\"removing\")\n",
    "            return True\n",
    "\n",
    "    print(\"keeping\")\n",
    "    return False\n",
    "\n",
    "# Also remove policies that include RfR after dike heightening\n",
    "result_constrained = result_constrained[result_constrained.apply(has_rfr_after_dh, axis=\"columns\") == False]\n",
    "result_constrained"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-16T11:37:12.789985Z",
     "end_time": "2023-06-16T11:37:12.832639Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove columns for outcomes from the dataframe, resulting in a df of polcies\n",
    "policies = result_constrained.drop([o.name for o in model.outcomes], axis=1)\n",
    "policies"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-16T11:37:12.795781Z",
     "end_time": "2023-06-16T11:37:12.832935Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hypervolume metric\n",
    "\n",
    "Here, there is an issue between the dike model and the EMA Workbench. The dike model defines value keys (levers, uncertainties, outcomes) that do not follow Python identifier standards, by including spaces or dots, or starting with digits.\n",
    "\n",
    "We avoid this by replacing disallowed symbols with allowed (but otherwise unused) symbols, Ç and Ñ. We also add the letter \"A\" in front of keys starting with digits."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "result_sanitized = optimization_result.copy()\n",
    "model_sanitized = deepcopy(model)\n",
    "\n",
    "# Here, we need to rename...\n",
    "def sanitize_as_python_identifier(x):\n",
    "    # Replace dots\n",
    "    x = x.replace(\".\",\"Ç\")\n",
    "    # Replace spaces\n",
    "    x = x.replace(\" \",\"Ñ\")\n",
    "    # Add letter if starts with digit\n",
    "    if x.startswith((\"0\",\"1\",\"2\",\"3\",\"4\",\"5\")):\n",
    "        x = \"A\" + x\n",
    "    return x\n",
    "\n",
    "# Reverse functions for if we need to get the original labels...\n",
    "def desanitize_as_python_identifier(x):\n",
    "    # Replace dots\n",
    "    x = x.replace(\"Ç\",\".\")\n",
    "    # Replace spaces\n",
    "    x = x.replace(\"Ñ\",\" \")\n",
    "    # Add letter if starts with digit\n",
    "    if x.startswith((\"A0\",\"A1\",\"A2\",\"A3\",\"A4\",\"A5\")):\n",
    "        x = x[1:]\n",
    "    return x\n",
    "\n",
    "result_sanitized.columns = [sanitize_as_python_identifier(x) for x in result_sanitized.columns]\n",
    "\n",
    "for lev in model_sanitized.levers:\n",
    "    lev.name = sanitize_as_python_identifier(lev.name)\n",
    "\n",
    "for unc in model_sanitized.uncertainties:\n",
    "    unc.name = sanitize_as_python_identifier(unc.name)\n",
    "\n",
    "for out in model_sanitized.outcomes:\n",
    "    out.name = sanitize_as_python_identifier(out.name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-16T11:37:12.799954Z",
     "end_time": "2023-06-16T11:37:12.843521Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load data from archive\n",
    "archives = ArchiveLogger.load_archives(\"./MORDM_Data/MORDM_HV_1.tar.gz\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-16T11:37:12.807091Z",
     "end_time": "2023-06-16T11:37:12.902099Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "problem = to_problem(model_sanitized, searchover=\"levers\")\n",
    "\n",
    "hv = HypervolumeMetric(result_sanitized, problem)\n",
    "\n",
    "print(f\"Going over {len(archives.items())} archives...\")\n",
    "hypervolume = []\n",
    "\n",
    "# As this iss a very slow process, we want to store the results at every step\n",
    "hypervolume_folder = f\"./Hypervolume_Data/{str(round(datetime.now().timestamp()))}\"\n",
    "Path(hypervolume_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for i, (nfe, archive) in enumerate(archives.items()):\n",
    "    with ProcessPoolExecutor(8) as pool:\n",
    "        print(datetime.now().strftime('%H:%M:%S') + \" - Hypervolume calculate for archive #\" + str(i))\n",
    "        archive_sanitized = archive\n",
    "        archive_sanitized.columns = [sanitize_as_python_identifier(x) for x in archive_sanitized.columns]\n",
    "        the_result = (nfe, hv.calculate(archive))\n",
    "        hypervolume.append(the_result)\n",
    "        filename = hypervolume_folder + \"/\" + f\"hv_data_count_{i}.json\"\n",
    "        with open(filename, \"w\") as fp:\n",
    "            json.dump(hypervolume, fp)\n",
    "        print(datetime.now().strftime('%H:%M:%S') + \" - Dumped hypervolume data into file \" + filename)\n",
    "        # can be read with open(\"test\", \"r\") - not implemented yet\n",
    "\n",
    "hypervolume.sort(key=lambda x:x[0])\n",
    "hypervolume = np.asarray(hypervolume)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-16T11:37:12.864869Z",
     "end_time": "2023-06-16T11:38:23.938169Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plot hypervolume and epsilon progress"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharex=True, figsize=(8,4))\n",
    "ax1.plot(optimization_convergence.nfe, optimization_convergence.epsilon_progress)\n",
    "ax1.set_ylabel('$\\epsilon$-progress')\n",
    "ax2.plot(hypervolume[:, 0], hypervolume[:, 1])\n",
    "ax2.set_ylim(ymin=0)\n",
    "ax2.set_ylabel('hypervolume')\n",
    "\n",
    "ax1.set_xlabel('number of function evaluations')\n",
    "ax2.set_xlabel('number of function evaluations')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-16T11:38:23.939475Z",
     "end_time": "2023-06-16T11:38:24.216594Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plot the initial pool of solutions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from ema_workbench.analysis import parcoords\n",
    "\n",
    "data = optimization_result.loc[:, [o.name for o in model.outcomes]]\n",
    "limits = parcoords.get_limits(data)\n",
    "\n",
    "paraxes = parcoords.ParallelAxes(limits)\n",
    "paraxes.plot(data)\n",
    "plt.title(\"All scenarios\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-16T11:38:24.223572Z",
     "end_time": "2023-06-16T11:38:25.127384Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plot optimized solutions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-16T11:38:25.146743Z",
     "end_time": "2023-06-16T11:38:25.232909Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "data = result_constrained.loc[:, [o.name for o in model.outcomes]]\n",
    "limits = parcoords.get_limits(data)\n",
    "\n",
    "paraxes = parcoords.ParallelAxes(limits)\n",
    "paraxes.plot(data)\n",
    "plt.title(\"Constrained scenarios\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
